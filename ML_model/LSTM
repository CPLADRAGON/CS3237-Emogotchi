import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# --- 1. Load and Preprocess Data ---

# Load the dataset you already created
df = pd.read_csv('simulated_emogotchi_dataset_v3.csv')

# Define Features (X) and Target (y)
features = ['hrv', 'noise_db', 'movement', 'light_lux', 'temperature_c']
target = 'Emotion'

# --- Scale numerical features ---
# Neural networks are very sensitive to the scale of data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[features])

# --- Encode string labels to numbers ---
# "happy" -> 0, "plain" -> 1, "sad" -> 2
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(df[target])

# Convert numerical labels to one-hot encoding
# 0 -> [1, 0, 0]
# 1 -> [0, 1, 0]
# 2 -> [0, 0, 1]
y_categorical = to_categorical(y_encoded)


# --- 2. Create "Windowed" Sequences ---

# This function converts our flat data into 3D sequences
def create_sequences(X, y, time_steps=10):
    """
    Creates sequences of a fixed 'time_steps' length.
    X: The input features (scaled)
    y: The target labels (one-hot encoded)
    time_steps: How many previous rows to look at for one prediction.
    """
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        # Get a "window" of 'time_steps' rows
        Xs.append(X[i:(i + time_steps)])
        # Get the label at the *end* of that window
        ys.append(y[i + time_steps])
    return np.array(Xs), np.array(ys)

# --- Define sequence length ---
# Let's say we'll look at 10 data points (e.g., 10 seconds) to make one prediction
TIME_STEPS = 10 

# Create the sequences
X_seq, y_seq = create_sequences(X_scaled, y_categorical, TIME_STEPS)

print(f"Original features shape: {X_scaled.shape}")
print(f"Sequenced features shape: {X_seq.shape}") # (samples, timesteps, features)
print(f"Sequenced labels shape: {y_seq.shape}")


# --- 3. Split into Training and Testing Sets ---
# We split our *sequenced* data
X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)

n_features = X_seq.shape[2] # Should be 5
n_classes = y_seq.shape[1]  # Should be 3


# --- 4. Build the LSTM Model ---
print("Building LSTM model...")
model = Sequential()

# Input LSTM layer
# input_shape is (timesteps, features)
model.add(LSTM(units=64, return_sequences=True, input_shape=(TIME_STEPS, n_features)))
model.add(Dropout(0.2))

# A second LSTM layer
model.add(LSTM(units=32))
model.add(Dropout(0.2))

# A standard dense layer
model.add(Dense(units=16, activation='relu'))

# Output layer
# Must have 'n_classes' (3) units and 'softmax' activation
model.add(Dense(units=n_classes, activation='softmax'))

# Compile the model
model.compile(
    loss='categorical_crossentropy', # For one-hot encoded labels
    optimizer='adam',
    metrics=['accuracy']
)

model.summary()


# --- 5. Train the Model ---
print("Training LSTM model...")
history = model.fit(
    X_train, y_train,
    epochs=20,
    batch_size=32,
    validation_split=0.1,  # Use part of training data for validation
    shuffle=False # For time series, it's often better not to shuffle
)

# --- 6. Evaluate the Model ---
print("Evaluating model...")
loss, accuracy = model.evaluate(X_test, y_test)
print(f"\nModel Test Accuracy: {accuracy * 100:.2f}%")


# --- 7. How to Use This Model (Prediction) ---
print("\n--- Prediction Example ---")

# Let's grab one sequence from the test set to simulate new data
sample_sequence = X_test[0] 
actual_label_encoded = y_test[0]

# The model expects a "batch" of data, so we add one dimension
sample_sequence_batch = np.expand_dims(sample_sequence, axis=0)

# Make a prediction
prediction_probabilities = model.predict(sample_sequence_batch)[0]

# Get the *index* of the highest probability
predicted_index = np.argmax(prediction_probabilities)

# Turn the index (e.g., 2) back into the string label ("sad")
predicted_emotion = label_encoder.inverse_transform([predicted_index])[0]
actual_emotion = label_encoder.inverse_transform([np.argmax(actual_label_encoded)])[0]

print(f"Data sequence input shape: {sample_sequence_batch.shape}")
print(f"Model output (probabilities): {prediction_probabilities}")
print(f"Predicted emotion: {predicted_emotion}")
print(f"Actual emotion: {actual_emotion}")